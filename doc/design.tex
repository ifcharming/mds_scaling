\section{Design and implementation}

\begin{figure}[t]   %% START_FIGURE
\centerline{\includegraphics[scale=0.3]{./figs/giga-impl-leveldb-clusterfs}}
\caption{\normalsize
%\textbf{Proposed design.}
Design of our scalable metadata middleware that integrates a distributed metadata indexing
technique with a tabular metadata-optimized on-disk layout on each server and
layers on existing cluster file systems. 
%Our approach for a scalable metadata service integrates two components: a highly 
%parallel and load-balanced indexing technique (called \giga{} \cite{GIGA}) to 
%partition
%metadata over multiple servers and an optimized metadata representation (called
%\tfs{} \cite{TableFS}) on each server. 
%Our approach aims to layer this integrated solution on top of existing cluster file 
%system deployments. 
}
\vspace{10pt}
\hrule 
\label{fig:design}
\end{figure}       %% END_FIGURE

Figure \ref{fig:design} shows the architecture of our scalable metadata
service that is designed to be layered on existing deployments of cluster file
systems. Our approach uses a client-server architecture and has three components: 
unmodified applications running on clients, the \giga{} directory indexing service 
on clients and servers, and the \ldb{}-based persistent metadata representation 
managed by the server.
Applications interact with our middleware using the VFS interface exposed
through the FUSE user-level file system \cite{fuse}.
All metadata requests, such as \texttt{create()}, \texttt{mkdir()} and
\texttt{open()}, are handled through the \giga indexing modules that address
the request to the appropriate server.
Each indexing server manages its local \ldb instance to store and access all
metadata information. This \ldb instance stores flat files (in its special
format) containing changes in metadata. 
Once the client receives the relevant metadata back from the server, our
middleware allows clients to access the actual file contents directly through
the cluster file system.

Using \giga{} and \ldb{} enables us to tackle two key challenges: highly 
concurrent metadata distribution for ingest-intensive parallel applications
such as checkpointing \cite{PLFS} and 
optimized metadata representation that stores all file system
metadata in structured, indexed files managed by existing cluster file system
deployments \cite{LevelDB}. 

Remainder of this section describes more details of our approach. 
Section \ref{design.giga} presents a primer on how \giga{} distributes 
metadata. 
Section \ref{design.tablefs} shows how \ldb{} stores all file system metadata
using a single on-disk structure on each server. 
Section \ref{design.integration} describes the challenges in effectively
integrating \giga{} and \ldb{} to work with existing cluster file systems.

\subsection{Scalable partitioning using \giga{}}
\label{design.giga}
\input{giga}

\subsection{Metadata layout using \ldb{}}
\label{design.tablefs}
\input{tablefs}

\subsection{Integrating \giga{} and \ldb{}}
\label{design.integration}

To effectively integrate the \giga{} distribution mechanism with the
\ldb{}-based metadata representation, we tackled several challenges. 

~\\
\textbf{Metadata representation -- }
\ldb{} stores all metadata including \giga{} hash
partitions for directories, entries in each hash partition, and other
bootstrapping information such as root entry and \giga{} configuration state.
The general schema used to store all file is:
%\texttt{\{key\} --> \{value\}} format:

\begin{verbatim}
    <KEY>         -->     <VALUE> 

{parentDirID,         {attr(dirEntry),
 gigaPartitionID, -->  symlink,
 hash(dirEntry),       gigaMetaState}
 dirEntry}
\end{verbatim}

The main differences from the \ldb{} schema described in Section
\ref{design.tablefs} is the addition of \texttt{gigaPartitionID} to identify a
\giga{} hash partition and the optional \texttt{gigaMetaState} to store the
hash partition related mapping information. \giga{} related fields are used
only if large directories are distributed over multiple metadata servers.\footnote{
Since we already store the \texttt{hash} of the directory entry, we can use the
hash-values to identify hash partitions if we chose to use the same hash
function for both \giga and \ldb keys. This optimization can eliminate the
need for \texttt{gigaPartitionID} in the schema.} 

~\\
\textbf{Partition splitting -- }
Each \giga{} hash partition and its directory entries are stored in 
SSTable files in a local \ldb{} instance. 
Recall that each \giga{} server process splits a hash partition $P$ on 
overflow and creates another hash partition $P'$ which is managed by a 
different server; this split involves migrating approximately half the entries 
from old partition $P$ to the new hash partition $P'$ on another server during
which the key range in write is locked.
We explored several ways to perform this cross-server partition split.

A simple approach to splitting would be to perform a \ldb range scan on 
partition $P$ and deleting about half the results (corresponding to the keys
that are migrated to the new partition) from $P$. 
All entries that need to be moved to the new partition $P'$ are batched
together and sent in an RPC message to the server that will manage partition 
$P'$.
The recipient server inserts each key in the batch in its own \ldb{}
instance. While the simplicity of this approach makes it attractive, we would
like a faster technique to reduce the time that the range is write locked. 

The immutability of \ldb SSTables makes such a fast bulk insert possible -- an
SSTable can be added to Level 0 without its data being pushed through the
write-ahead log and minor compaction process.
To take advantage of this opportunity, we extended \ldb{}
to support a three-phase split operation. 
First, the split initiator performs a range scan on its \ldb{} instance to find all
entries in the hash-range that needs to be moved to another server. The results
of this scan are written in a \ldb{}-specific SSTable format to file in the
underlying cluster file system. 
In the second step, the split initiator notifies the split receiver about
the new \ldb{}-format file in a much smaller RPC message.
The split receiver then bulk inserts the file into the \ldb{} tree structure 
instead of iteratively inserting one key at a time.
The final step is a clean-up and commit phase: after the receiver completes the 
bulk insert operation, it notifies the 
initiator, who then deletes the migrated hash-range from its LevelDB instance
and unlocks the range.%\footnote{
%This three-phase split can be refined even further: \ldb{} can use symbolic links 
%to these split files without explicitly copying the files through shared
%storage. Because the current release of \ldb{} does not have support for links, we 
%left this optimization for future work. 
%}

~\\
\textbf{Decoupled data and metadata path -- }
All metadata operations go through the \giga{} server; however, following the
same path for data operations would incur an unnecessary performance penalty 
of shipping data over the network on extra time. 
This penalty can be significant in HPC use-cases where files can easily be  
gigabytes to terabytes in size.

To avoid this penalty our middleware is designed to perform all
data-path operations directly through the cluster file system module in client
machine. 
Figure \ref{fig:design} illustrates this data path (in BLUE color).
Once the client completes a
lookup on a desired file name, it gets back a symbolic link to the physical
path in the cluster file system. All subsequent accesses using this symbolic
link force the client operating system to resolve this link into the cluster
file system.
While the file is open, some of its attributes (e.g., file size and last access time)
may change relative to \ldb's per-open
copy of the attributes. \giga will capture these changes on file close on the
metadata path. Other attribute changes relatvie to permissions can be updated on-flight
through \giga servers. 

%\subsubsection*{Other challenges.}

