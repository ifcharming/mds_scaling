\section{Design and implementation}

\begin{figure}[t]   %% START_FIGURE
\centerline{\includegraphics[scale=0.3]{./figs/giga-impl-leveldb-clusterfs}}
\caption{
%\textbf{Proposed design.}
{\small
Design of our scalable metadata middleware that integrates a distributed metadata indexing
technique with a tabular metadata-optimized on-disk layout on each server and
layers on existing cluster file systems. 
%Our approach for a scalable metadata service integrates two components: a highly 
%parallel and load-balanced indexing technique (called \giga{} \cite{GIGA}) to 
%partition
%metadata over multiple servers and an optimized metadata representation (called
%\tfs{} \cite{TableFS}) on each server. 
%Our approach aims to layer this integrated solution on top of existing cluster file 
%system deployments. 
}
}
\vspace{10pt}
\hrule 
\label{fig:design}
\end{figure}       %% END_FIGURE

Figure \ref{fig:design} shows the architecture of our scalable metadata
service that is designed to be layered on existing deployments of cluster file
systems. Our approach uses a client-server architecture and has three components: 
unmodified applications running on clients, the \giga{} directory indexing service 
on clients and servers, and the \ldb{}-based persistent metadata representation 
managed by the server.
Applications interact with our middleware using the VFS interface exposed
through the FUSE user-level file system \citep{fuse}.
All metadata requests, such as \texttt{create()}, \texttt{mkdir()} and
\texttt{open()}, are handled through the \giga indexing modules that address
the request to the appropriate server.
Each indexing server manages its local \ldb instance to store and access all
metadata information. This \ldb instance is stored in the underlying cluster
file system.
Once the client receives the relevant metadata back from the server, our
middleware allows clients to access the actual file contents directly through
the cluster file system.

Using \giga{} and \ldb{} enables us to tackle two key challenges: highly 
concurrent metadata distribution for ingest-intensive parallel applications
such as checkpointing \cite{PLFS} and 
optimized metadata representation that stores all file system
metadata in structured, indexed files manages by existing cluster file system
deployments \cite{LevelDB}. 
Remainder of this section describes more details of our approach. 
Section \ref{design.giga} presents a primer on how \giga{} distributes the
metadata. 
Section \ref{design.tablefs} shows how \ldb{} stores all file system metadata
using a single on-disk structure on each server. 
Section \ref{design.integration} describes the challenges in effectively
integrating \giga{} and \ldb{} to work with existing cluster file systems.

\subsection{Scalable partitioning using \giga{}}
\label{design.giga}
\input{giga}

\subsection{Metadata layout using \ldb{}}
\label{design.tablefs}
\input{tablefs}

\subsection{Integrating \giga{} and \ldb{}}
\label{design.integration}

To effectively integrate the \giga{} distribution mechanism with the
\ldb{}-based metadata representation, we tackled several challenges. 

\subsubsection*{Metadata representation.}

\ldb{} stores all metadata in the system including \giga{} hash
partitions for all directories, entries in each hash partition, and other
bootstrapping information such as root entry and \giga{} configuration state.
The general schema used to store all file is shown below in
\texttt{\{key\} --> \{value\}} format:

\begin{verbatim}
{parentDirID,         {attr(dirEntry),
 gigaPartitionID, -->  symlink/data,
 hash(dirEntry)}       gigaMetaState}
\end{verbatim}

The main differences from the \ldb{} schema described in Section
\ref{design.tablefs} is the addition of \texttt{gigaPartitionID} to identify a
\giga{} hash partition and the optional \texttt{gigaMetaState} to store the
hash partition related mapping information. \giga{} related fields are used
only if large directories are distributed over multiple metadata servers.
\footnote{
Since we already store the \texttt{hash} of the directory entry, we can use the
hash-values to identify hash partitions; this optimization will eliminate the
need for \texttt{gigaPartitionID} in the schema.} 

\subsubsection*{Partition splitting.}

Each \giga{} hash partition and its directory entries are stored in sorted
SSTable files in local \ldb{} instance. 
Recall that each \giga{} server process manages its hash partition $P$ that, on 
overflow, is split into another hash partition $P'$ which is stored on a 
different server; this split involves migrating approximately half the entries 
from old partition $P$ to the new hash partition $P'$ on to another server. 
We explored several ways to perform this cross-server partition split.

One approach would be to perform a range scan on partition $P$ and for each
entry in the scan result, check if it needs to be moved to a different server.
All entries that need to be moved to the new partition $P'$ are batched
together and sent in an RPC message to the server that manages partition $P'$.
The recipient server scans the batch and inserts each key in its own \ldb{}
instance. While the simplicity of this approach makes it an attractive
alternative, there is significant cost stemming from \ldb{} compaction
operations: inserts in \ldb{} tables will force merging and rearranging which
degrades performance until it completes.

To minimize the overhead of these background compactions, we extended \ldb{}
to support a three-phase split operation. 
First, the split initiator performs a range scan on its \ldb{} instance to find all
entries in the hash-range that needs to be moved to another server. The results
of this scan are written to a separate \ldb{}-specific SSTable format file in a 
shared storage volume accessible to all \giga{} servers.
In the second step, the split initiator notifies the split receiver about this new
partition split and the pointer to the new \ldb{}-format file in the shared
storage volume. The split receiver reads this file and performs a bulk
insertion in the \ldb{} table. This bulk insertion can
directly ``plug in'' the file in the \ldb{} tree structure instead of
iteratively inserting one key at a time.
The final step is a clean-up and commit phase: after the receiver completes the 
bulk insert operation, it empties the shared store volume and notifies the 
initiator, who then deletes the hash-range from its LevelDB instance.
\footnote
{
This three-phase split can be refined even further: \ldb{} can use symbolic links 
to these split files without explicitly copying the files through shared
storage. Because the current release of \ldb{} does not have support for links, we 
left this optimization for future work. 
}

\subsubsection*{Decouple data and metadata path.}
All metadata operations go through the \giga{} server; however, following the
same path for data operations would incur a performance penalty from shipping
data over the network and copying data from user-mode to kernel-mode. And this
penalty with be significant in HPC use-cases where file can easily gigabytes of
more in size. To avoid, this penalty our middleware is designed to perform all
data-path operations directly through the clients. Once the client completes a
lookup on a desired file name, it gets back a symbolic link to the physical
path in the cluster file system. All subsequent access using this symbolic
link forces VFS to resolve this link and pass the request using the cluster
file system client module running on the same node as the application. Figure
\ref{fig:design} illustrates this data path (in BLUE color).

%\subsubsection*{Other challenges.}

